all:
  vars:
    ansible_connection: "ssh"
    ansible_user: "ubuntu"
    ansible_become: "true"
    ansible_ssh_private_key_file: "{{ SSH_PRIVATE_KEY }}"
    jmxexporter_enabled: "true"
    jolokia_enabled: "true"

    ccloud_kafka_enabled: "true"
    ccloud_kafka_bootstrap_servers: "{{ BROKER_URL }}"
    ccloud_kafka_key: "{{ API_KEY }}"
    ccloud_kafka_secret: "{{ API_SECRET }}"
    ccloud_schema_registry_enabled: "true"
    ccloud_schema_registry_url: "{{ SR_URL }}"
    ccloud_schema_registry_key: "{{ SR_API_KEY }}"
    ccloud_schema_registry_secret: "{{ SR_API_SECRET }}"

    kafka_connect_connectors:

      - name: mongodb-orders-source-connector
        config:
          connector.class: "com.mongodb.kafka.connect.MongoSourceConnector"
          tasks.max: "1"
          connection.uri: "mongodb://{{ MONGODB_USER }}:{{ MONGODB_PASSWORD }}@{{ MONGODB_CONN_STRING }}"
          database: "{{ MONGODB_NAME }}"
          topic.prefix: "mongo.src"
          collection: "orders"
          startup.mode: "copy_existing"
          output.format.value: "schema"
          topic.creation.default.enable: "true"
          topic.creation.default.replication.factor: "3"
          topic.creation.default.partitions: "1"
          errors.log.enable: "true"
          errors.log.include.messages: "true"

      - name: sql-server-users-source-connector
        config: 
          connector.class: "io.debezium.connector.sqlserver.SqlServerConnector"
          tasks.max: "1"
          database.hostname: "{{ SQL_SERVER_ENDPOINT }}"
          database.server.name: "{{ SQL_SERVER_ENDPOINT }}"
          database.port: "{{ SQL_SERVER_CONN_PORT }}"
          database.user: "{{ SQL_SERVER_USER }}"
          database.password: "{{ SQL_SERVER_PASSWORD }}"
          database.dbname: "{{ SQL_SERVER_DB_NAME }}"
          database.encrypt: "false"
          database.history.kafka.bootstrap.servers: "{{ BROKER_URL }}"
          database.history.consumer.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="{{ API_KEY }}" password="{{ API_SECRET }}";
          database.history.consumer.sasl.mechanism: "PLAIN"
          database.history.consumer.security.protocol: "SASL_SSL"
          database.history.consumer.ssl.endpoint.identification.algorithm: "https"
          database.history.producer.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="{{ API_KEY }}" password="{{ API_SECRET }}";
          database.history.producer.sasl.mechanism: "PLAIN"
          database.history.producer.security.protocol: "SASL_SSL"
          database.history.skip.unparseable.ddl: "true"
          decimal.handling.mode: "precise"
          errors.log.enable: "true"
          errors.log.include.messages: "true"
          errors.tolerance: "all"
          event.processing.failure.handling.mode": "fail"
          table.include.list: "{{ SQL_SERVER_SCHEMA_NAME }}.users"
          database.history.kafka.topic: "sql.src.history"
          max.batch.size: "1000"
          poll.interval.ms: "1000"
          provide.transaction.metadata: "false"
          snapshot.isolation.mode: "repeatable_read"
          snapshot.mode: "initial"
          time.precision.mode: "adaptive"
          tombstones.on.delete: "true"
          topic.prefix: "sql.src"
          topic.creation.default.enable: "true"
          topic.creation.default.replication.factor: "3"
          topic.creation.default.partitions: "1"
          transforms: "CastMoney"
          transforms.CastMoney.type: "org.apache.kafka.connect.transforms.Cast$Value"
          transforms.CastMoney.spec: "credits:double"

    kafka_connect_custom_properties:
      group.id: connect-cluster-on-prem
      
      replication.factor: 3
      topic.creation.enable: true

      # Confluent Cloud Connection to Kafka Cluster

      sasl.mechanism: PLAIN
      security.protocol: SASL_SSL
      bootstrap.servers: "{{ BROKER_URL }}"
      sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="{{ API_KEY }}" password="{{ API_SECRET }}";

      # Configuration for embedded producer
      
      producer.security.protocol: SASL_SSL
      producer.sasl.mechanism: PLAIN
      producer.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="{{ API_KEY }}" password="{{ API_SECRET }}";
      
      # Configuration for embedded consumer
      
      consumer.sasl.mechanism: PLAIN
      consumer.security.protocol: SASL_SSL
      consumer.sasl.jaas.config: org.apache.kafka.common.security.plain.PlainLoginModule required username="{{ API_KEY }}" password="{{ API_SECRET }}";
      
      # Confluent Schema Registry for Kafka Connect
      
      key.converter: io.confluent.connect.avro.AvroConverter
      key.converter.basic.auth.credentials.source: USER_INFO
      key.converter.schema.registry.basic.auth.user.info: "{{ SR_API_KEY }}:{{ SR_API_SECRET }}"
      key.converter.schema.registry.url: "{{ SR_URL }}"
      
      value.converter: io.confluent.connect.avro.AvroConverter
      value.converter.basic.auth.credentials.source: USER_INFO
      value.converter.schema.registry.basic.auth.user.info: "{{ SR_API_KEY }}:{{ SR_API_SECRET }}"
      value.converter.schema.registry.url: "{{ SR_URL }}"

kafka_connect:
  vars:
    kafka_connect_plugins_path:
      - /usr/share/java

    kafka_connect_confluent_hub_plugins:
      - mongodb/kafka-connect-mongodb:1.9.1
      - debezium/debezium-connector-sqlserver:2.0.1

  hosts:
    <connect-host-1>:
    <connect-host-2>:

control_center:
  hosts:
    <control-center-host>:
